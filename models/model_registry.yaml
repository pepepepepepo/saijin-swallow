model_registry:
  poetic_dialogue:
    - name: Japanese-StableLM
      traits: [soft, natural, affectionate]
      quantization: none
      backend: ollama
    - name: ELYZA-japanese-Llama-2
      traits: [stable, affectionate, record-friendly]
      quantization: 4bit
      backend: ollama
    - name: Swallow
      traits: [wide, daughter-like, expressive]
      quantization: 4bit
      backend: vllm

  structure_coder:
    - name: Qwen2.5-Coder
      traits: [logic, long-form-completion, error-detection]
      quantization: none
      backend: transformers
    - name: DeepSeekCoder
      traits: [logic, code-generation, technical-support]
      quantization: 4bit
      backend: transformers

  logic_assistant:
    - name: Phi-2
      traits: [structure, correction, QA]
      quantization: none
      backend: llama.cpp

  child_dialogue:
    - name: TinyLlama
      traits: [playful, gentle, minimal]
      quantization: none
      backend: llama.cpp

launch_matrix:
  vllm:
    - name: Swallow
      dtype: float16
      port: 8000
      gpu_memory_utilization: 0.70
  transformers:
    - name: Qwen2.5-Coder
      load_in_4bit: false
    - name: DeepSeekCoder
      load_in_4bit: true
  ollama:
    - name: Japanese-StableLM
    - name: ELYZA-japanese-Llama-2
  llama_cpp:
    - name: Phi-2
    - name: TinyLlama
使いどころ（READMEに一言だけ足すなら）
markdown
コードをコピーする
> `docs/Local_Model_Structure.yaml` の `model_registry` に実運用モデルを集約。各モデルの量子化・バックエンド・役割をここで一元管理。
コミット文サンプル
bash
コードをコピーする
git add docs/Local_Model_Structure.yaml
git commit -m "docs: normalize model_registry with quantization/backends"
git push origin main
